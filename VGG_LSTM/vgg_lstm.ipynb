{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\ksaqu\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from keras.layers import Conv3D, MaxPooling3D\n",
    "from keras.layers import Dense, Dropout, Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, ZeroPadding3D, TimeDistributed, LSTM, GRU, Reshape\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "import gc\n",
    "from sklearn.metrics import classification_report, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.preprocessing import label_binarize\n",
    "from itertools import cycle\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import sklearn\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from prettytable import PrettyTable\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "def extract_frames(video_folder, output_folder):\n",
    "    # Create output folder if it doesn't exist\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    # Iterate through each video file in the folder\n",
    "    for filename in os.listdir(video_folder):\n",
    "        if filename.endswith(\".mp4\") or filename.endswith(\".avi\"):\n",
    "            video_path = os.path.join(video_folder, filename)\n",
    "            video_name = os.path.splitext(filename)[0]\n",
    "\n",
    "            # Create a subfolder for each video\n",
    "            video_output_folder = os.path.join(output_folder, video_name)\n",
    "            if not os.path.exists(video_output_folder):\n",
    "                os.makedirs(video_output_folder)\n",
    "\n",
    "            # Open the video file\n",
    "            cap = cv2.VideoCapture(video_path)\n",
    "            frame_count = 0\n",
    "\n",
    "            # Read and process each frame\n",
    "            while(cap.isOpened()):\n",
    "                ret, frame = cap.read()\n",
    "                if not ret:\n",
    "                    break\n",
    "                \n",
    "                # Perform your processing here to identify words and numbers in the frame\n",
    "                # For simplicity, let's just save every 16th frame\n",
    "                if frame_count % 16 == 0:\n",
    "                    output_frame_path = os.path.join(video_output_folder, f\"{video_name}_frame_{frame_count}.jpg\")\n",
    "                    cv2.imwrite(output_frame_path, frame)\n",
    "\n",
    "                frame_count += 1\n",
    "\n",
    "            cap.release()\n",
    "\n",
    "# Example usage:\n",
    "video_dataset_folder = \"D:\\\\New Dataset Lip Movement Projec\\\\FINAL YEAR PROJECT\\\\Dataset2.0\"\n",
    "output_frames_folder = \"D:\\\\New Dataset Lip Movement Projec\\\\FINAL YEAR PROJECT\\\\Frames\"\n",
    "extract_frames(video_dataset_folder, output_frames_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG MODEL BUILDING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 127 images belonging to 17 classes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 22 images belonging to 17 classes.\n",
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_14 (InputLayer)       [(None, 96, 72, 3)]       0         \n",
      "                                                                 \n",
      " vgg16 (Functional)          (None, 3, 2, 512)         14714688  \n",
      "                                                                 \n",
      " reshape_6 (Reshape)         (None, 3, 1024)           0         \n",
      "                                                                 \n",
      " lstm_12 (LSTM)              (None, 3, 64)             278784    \n",
      "                                                                 \n",
      " lstm_13 (LSTM)              (None, 16)                5184      \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 17)                289       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 14998945 (57.22 MB)\n",
      "Trainable params: 284257 (1.08 MB)\n",
      "Non-trainable params: 14714688 (56.13 MB)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Flatten, Reshape\n",
    "from tensorflow.keras.applications import VGG16\n",
    "\n",
    "# Hyperparameters (adjust as needed)\n",
    "IMAGE_SIZE = (96, 72)  # Assuming your image size is 112x80\n",
    "BATCH_SIZE = 16\n",
    "EPOCHS = 100\n",
    "NUM_CLASSES = 17\n",
    "TRAIN_DATA_DIR = \"D:\\\\New Dataset Lip Movement Projec\\\\FINAL YEAR PROJECT\\\\Frames\"\n",
    "\n",
    "\n",
    "\n",
    "# Define data generator\n",
    "datagen = tf.keras.preprocessing.image.ImageDataGenerator(\n",
    "    #rescale=1./255,\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Load and preprocess the data with splitting\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training'  # 80% of the data will be used for training\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    TRAIN_DATA_DIR,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'  # 20% of the data will be used for validation\n",
    ")\n",
    "\n",
    "# Load pre-trained VGG-16 model, excluding the top classifier layers\n",
    "base_model = VGG16(weights='imagenet', include_top=False, input_shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))\n",
    "\n",
    "# Freeze pre-trained layers to prevent them from being updated during training\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Define the model architecture\n",
    "inputs = Input(shape=(IMAGE_SIZE[0], IMAGE_SIZE[1], 3))  # Input layer for images\n",
    "\n",
    "# Extract features using VGG-16\n",
    "vgg_features = base_model(inputs)\n",
    "\n",
    "# **Reshape features to ensure compatibility with LSTM (remove unnecessary dimension):**\n",
    "# Adjust the shape based on the actual dimensions of `vgg_features` (e.g., if features are (None, 7, 7, 512), reshape to (None, 7, 512))\n",
    "desired_feature_shape = (vgg_features.shape[1], vgg_features.shape[2] * vgg_features.shape[3])  # Calculate desired shape\n",
    "reshaped_features = Reshape(desired_feature_shape)(vgg_features)\n",
    "\n",
    "# Define the LSTM-based classifier\n",
    "lstm_1 = LSTM(64, return_sequences=True)(reshaped_features)  # First LSTM layer\n",
    "lstm_2 = LSTM(16)(lstm_1)                                     # Second LSTM layer\n",
    "\n",
    "Dropout(0.7)\n",
    "# Final dense layer for classification\n",
    "predictions = Dense(NUM_CLASSES, activation='softmax')(lstm_2)\n",
    "\n",
    "# Create the final model\n",
    "model = tf.keras.Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "model.compile(optimizer=tf.keras.optimizers.SGD(clipnorm=1),  # Create Adam optimizer with clipnorm\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'],\n",
    "              run_eagerly=True,  # Enables early stopping callbacks\n",
    "              )\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n",
    "\n",
    "# Train and evaluate your model\n",
    "# ... (Train your model using train_generator and evaluate using validation_generator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KerasTensor(type_spec=TensorSpec(shape=(None, 3, 2, 512), dtype=tf.float32, name=None), name='vgg16/block5_pool/MaxPool:0', description=\"created by layer 'vgg16'\")\n"
     ]
    }
   ],
   "source": [
    "print(vgg_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "7/7 [==============================] - 10s 1s/step - loss: 2.8418 - accuracy: 0.0360 - val_loss: 2.7860 - val_accuracy: 0.3125\n",
      "Epoch 2/100\n",
      "7/7 [==============================] - 7s 1s/step - loss: 2.8171 - accuracy: 0.0625 - val_loss: 2.7735 - val_accuracy: 0.2500\n",
      "Epoch 3/100\n",
      "7/7 [==============================] - 7s 1s/step - loss: 2.7847 - accuracy: 0.0901 - val_loss: 2.7782 - val_accuracy: 0.2500\n",
      "Epoch 4/100\n",
      "7/7 [==============================] - 7s 1s/step - loss: 2.7673 - accuracy: 0.1171 - val_loss: 2.7897 - val_accuracy: 0.1875\n",
      "Epoch 5/100\n",
      "7/7 [==============================] - 7s 1s/step - loss: 2.7543 - accuracy: 0.1351 - val_loss: 2.8216 - val_accuracy: 0.1250\n",
      "Epoch 6/100\n",
      "7/7 [==============================] - 7s 1s/step - loss: 2.7435 - accuracy: 0.1518 - val_loss: 2.7536 - val_accuracy: 0.1875\n",
      "Epoch 7/100\n",
      "7/7 [==============================] - ETA: 0s - loss: 2.7257 - accuracy: 0.1802"
     ]
    }
   ],
   "source": [
    "start_time=time.time()\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    steps_per_epoch=train_generator.samples // BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=validation_generator.samples // BATCH_SIZE\n",
    ")\n",
    "# Stop the timer\n",
    "end_time = time.time()\n",
    "\n",
    "# Compute the computational time\n",
    "comp_time = end_time - start_time\n",
    "\n",
    "model.save('vgg_lstm5.h5')\n",
    "\n",
    "# Evaluate the model\n",
    "train_loss, train_acc = model.evaluate(train_generator)\n",
    "val_loss, val_acc = model.evaluate(validation_generator)\n",
    "\n",
    "print(f'Training Accuracy: {train_acc * 100}')\n",
    "print(f'Validation Accuracy: {val_acc * 100}')\n",
    "print(f'Training time: {comp_time}')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
